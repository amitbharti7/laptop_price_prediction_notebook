{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fe95fa7",
   "metadata": {},
   "source": [
    "# Laptop Price Prediction\n",
    "\n",
    "This notebook performs a full analysis and regression model pipeline for laptop price prediction.\n",
    "\n",
    "**Contents:**\n",
    "\n",
    "- Data loading\n",
    "- Beginner, Intermediate, Advanced, Expert analyses\n",
    "- Feature engineering\n",
    "- Regression model with evaluation and feature importance\n",
    "\n",
    "*Dataset source:* `https://raw.githubusercontent.com/jhhalls/Data-Analysis/main/Datasets/laptop_data.csv`\n",
    "\n",
    "_Run each cell in order._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdd53f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import joblib\n",
    "print('Imports done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7171eb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from GitHub raw URL\n",
    "url = 'https://raw.githubusercontent.com/jhhalls/Data-Analysis/main/Datasets/laptop_data.csv'\n",
    "try:\n",
    "    df = pd.read_csv(url)\n",
    "    print('Loaded data from GitHub raw URL:', url)\n",
    "except Exception as e:\n",
    "    print('Could not download from internet in this environment.\\nPlease place the CSV in the working directory as `laptop_data.csv`.')\n",
    "    # fallback: try local file\n",
    "    df = pd.read_csv('laptop_data.csv')\n",
    "print('Shape:', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ebc6a8",
   "metadata": {},
   "source": [
    "## Beginner Level — Basic Data Understanding\n",
    "\n",
    "Answer the beginner tasks with code and short commentary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d33cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic overview\n",
    "df.info(), df.describe(include='all').T.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f004404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Unique laptop companies and company with highest number of models\n",
    "company_counts = df['Company'].value_counts()\n",
    "num_unique_companies = df['Company'].nunique()\n",
    "top_company = company_counts.idxmax()\n",
    "top_company_count = company_counts.max()\n",
    "num_unique_companies, top_company, top_company_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7e11ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Average price overall and by company\n",
    "avg_price_overall = df['Price'].mean()\n",
    "avg_price_by_company = df.groupby('Company')['Price'].mean().sort_values(ascending=False)\n",
    "avg_price_overall, avg_price_by_company.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e0e116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Most common TypeName and its average price\n",
    "most_common_type = df['TypeName'].value_counts().idxmax()\n",
    "avg_price_most_common_type = df[df['TypeName']==most_common_type]['Price'].mean()\n",
    "most_common_type, avg_price_most_common_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630455bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Distribution of screen sizes\n",
    "inches_counts = df['Inches'].value_counts().sort_index()\n",
    "inches_counts, inches_counts.idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ffe584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Min, max, avg laptop weight and companies with heaviest/lightest\n",
    "weight_stats = df['Weight'].agg(['min','max','mean'])\n",
    "# convert weight to numeric if string contains 'kg'\n",
    "def to_kg(x):\n",
    "    try:\n",
    "        return float(str(x).replace('kg','').strip())\n",
    "    except:\n",
    "        return np.nan\n",
    "df['Weight_num']=df['Weight'].apply(to_kg)\n",
    "company_heaviest = df.loc[df['Weight_num'].idxmax(),'Company']\n",
    "company_lightest = df.loc[df['Weight_num'].idxmin(),'Company']\n",
    "weight_stats, company_heaviest, company_lightest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dfe5e0",
   "metadata": {},
   "source": [
    "## Intermediate Level — Comparative Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f036303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average price by Operating System\n",
    "avg_price_by_os = df.groupby('OpSys')['Price'].mean().sort_values(ascending=False)\n",
    "avg_price_by_os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e4c503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze how RAM affects price\n",
    "# Normalize RAM column (it may be like '8GB' or numeric). We'll extract numbers.\n",
    "def parse_ram(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    s = str(x)\n",
    "    s = s.replace('GB','').replace('gb','').strip()\n",
    "    try:\n",
    "        return int(s)\n",
    "    except:\n",
    "        try:\n",
    "            return int(float(s))\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "# If RAM is already numeric, keep it\n",
    "if df['RAM'].dtype == object:\n",
    "    df['RAM_num'] = df['RAM'].apply(parse_ram)\n",
    "else:\n",
    "    df['RAM_num'] = df['RAM'].astype(int)\n",
    "\n",
    "ram_price = df.groupby('RAM_num')['Price'].mean().sort_index()\n",
    "ram_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6fdbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ScreenResolution influence\n",
    "res_price = df.groupby('ScreenResolution')['Price'].mean().sort_values(ascending=False)\n",
    "res_price.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7dea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU brands dominance\n",
    "# Extract brand from CPU string (e.g., 'Intel Core i5' -> 'Intel')\n",
    "def cpu_brand(s):\n",
    "    if pd.isna(s): return 'Unknown'\n",
    "    s = s.lower()\n",
    "    if 'intel' in s: return 'Intel'\n",
    "    if 'amd' in s: return 'AMD'\n",
    "    if 'apple' in s: return 'Apple'\n",
    "    return s.split()[0].title()\n",
    "\n",
    "df['CPU_brand'] = df['CPU'].apply(cpu_brand)\n",
    "cpu_counts = df['CPU_brand'].value_counts()\n",
    "cpu_avg_price = df.groupby('CPU_brand')['Price'].mean().sort_values(ascending=False)\n",
    "cpu_counts, cpu_avg_price.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5438d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 most expensive laptop models\n",
    "top5 = df.sort_values('Price', ascending=False).head(5)\n",
    "top5[['Company','TypeName','Inches','CPU','GPU','RAM','Memory','OpSys','Weight','Price']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5139c334",
   "metadata": {},
   "source": [
    "## Advanced Level — Deeper Analytical Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd9ba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between numeric variables\n",
    "# Ensure numeric columns\n",
    "for c in ['Inches','RAM_num','Weight_num','Price']:\n",
    "    print(c, df[c].dtype)\n",
    "corr_df = df[['Inches','RAM_num','Weight_num','Price']].dropna()\n",
    "corr = corr_df.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c2ba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract screen quality categories from ScreenResolution\n",
    "# Heuristic: look for '4k', '3840', '2k', '2560', 'full hd', 'fhd', 'hd'\n",
    "def screen_quality(s):\n",
    "    s = str(s).lower()\n",
    "    if '4k' in s or '3840' in s: return '4K'\n",
    "    if '2k' in s or '2560' in s: return '2K'\n",
    "    if 'full hd' in s or 'fhd' in s or '1920' in s: return 'Full HD'\n",
    "    if 'hd' in s or '1366' in s: return 'HD'\n",
    "    return 'Other'\n",
    "\n",
    "df['Screen_Quality'] = df['ScreenResolution'].apply(screen_quality)\n",
    "df.groupby('Screen_Quality')['Price'].agg(['count','mean']).sort_values('mean',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984c0988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaming vs Non-gaming based on GPU (heuristic: if GPU string contains 'intel' or 'integrated' -> non-gaming)\n",
    "def is_dedicated_gpu(g):\n",
    "    if pd.isna(g): return False\n",
    "    s = str(g).lower()\n",
    "    if 'intel' in s or 'integrated' in s or 'uhd' in s or 'hd graphics' in s: return False\n",
    "    return True\n",
    "\n",
    "df['Dedicated_GPU'] = df['GPU'].apply(is_dedicated_gpu)\n",
    "df.groupby('Dedicated_GPU')['Price'].agg(['count','mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd09652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightweight vs heavier laptops average price (<1.5 kg)\n",
    "df['Lightweight'] = df['Weight_num'] < 1.5\n",
    "df.groupby('Lightweight')['Price'].agg(['count','mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8619c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best performance-to-price using simple proxy: (CPU_rank + GPU_rank + RAM)/Price\n",
    "# We'll create rough ranks: CPU_brand preference + GPU dedicated + RAM\n",
    "# CPU rank: Intel/AMD/Apple heuristics, fallback 0\n",
    "cpu_rank_map = {'Apple':3, 'Intel':2, 'AMD':2}\n",
    "df['CPU_rank'] = df['CPU_brand'].map(cpu_rank_map).fillna(1)\n",
    "df['GPU_rank'] = df['GPU'].apply(lambda g: 2 if is_dedicated_gpu(g) else 1)\n",
    "df['Perf_Score'] = df['CPU_rank'] + df['GPU_rank'] + (df['RAM_num'].fillna(0)/4)\n",
    "df['Perf_per_Price'] = df['Perf_Score'] / df['Price']\n",
    "df.sort_values('Perf_per_Price', ascending=False).head(10)[['Company','CPU','GPU','RAM','Price','Perf_per_Price']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ed570d",
   "metadata": {},
   "source": [
    "## Expert Level — Predictive Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810da30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick preprocessing and modeling pipeline\n",
    "# We'll engineer a few useful features and train RandomForest or GradientBoosting.\n",
    "df_model = df.copy()\n",
    "\n",
    "# Feature engineering helper functions\n",
    "def parse_memory(mem):\n",
    "    # examples: '256GB SSD', '1TB HDD', '256GB SSD + 1TB HDD'\n",
    "    if pd.isna(mem): return ''\n",
    "    s = str(mem).lower()\n",
    "    # count total GB\n",
    "    total_gb = 0\n",
    "    parts = s.replace('tb',' TB').replace('gb',' GB').split('+')\n",
    "    for p in parts:\n",
    "        p = p.strip()\n",
    "        if 'tb' in p:\n",
    "            try:\n",
    "                total_gb += float(p.split('tb')[0].strip())*1024\n",
    "            except:\n",
    "                pass\n",
    "        elif 'gb' in p:\n",
    "            try:\n",
    "                total_gb += float(p.split('gb')[0].strip())\n",
    "            except:\n",
    "                pass\n",
    "    return total_gb\n",
    "\n",
    "df_model['Memory_GB'] = df_model['Memory'].apply(parse_memory)\n",
    "# Keep numeric RAM, weight\n",
    "df_model['RAM_num'] = df_model['RAM_num'].fillna(df_model['RAM'].apply(lambda x: parse_ram(x) if pd.notna(x) else np.nan))\n",
    "df_model['Weight_num'] = df_model['Weight_num']\n",
    "# Create simple target\n",
    "y = df_model['Price']\n",
    "\n",
    "# Select features\n",
    "features = ['Company','TypeName','Inches','Screen_Quality','CPU_brand','GPU','RAM_num','Memory_GB','OpSys','Weight_num','Dedicated_GPU']\n",
    "X = df_model[features].copy()\n",
    "\n",
    "# Simple train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print('Train shape:', X_train.shape, 'Test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf58f3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build preprocessing\n",
    "numeric_features = ['Inches','RAM_num','Memory_GB','Weight_num']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_features = ['Company','TypeName','Screen_Quality','CPU_brand','OpSys','Dedicated_GPU']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])\n",
    "\n",
    "# For GPU we will do simple label encoding via target frequency -> we'll convert to top N GPUs and 'other'\n",
    "top_gpus = X_train['GPU'].value_counts().nlargest(20).index.tolist()\n",
    "X_train['GPU_simple'] = X_train['GPU'].apply(lambda g: g if g in top_gpus else 'Other')\n",
    "X_test['GPU_simple'] = X_test['GPU'].apply(lambda g: g if g in top_gpus else 'Other')\n",
    "\n",
    "gpu_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('gpu', gpu_transformer, ['GPU_simple'])\n",
    "    ], remainder='drop'\n",
    ")\n",
    "\n",
    "# Model pipeline\n",
    "model = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                       ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ba5ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model (this may take time depending on data size)\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2, mae, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c2569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance: approximate via permutation importance (scikit-learn)\n",
    "from sklearn.inspection import permutation_importance\n",
    "res = permutation_importance(pipe, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "# map importance back to feature names from preprocessor\n",
    "# get column names\n",
    "ohe_cols = list(pipe.named_steps['preprocessor'].transformers_[1][1].named_steps['onehot'].get_feature_names_out(categorical_features))\n",
    "gpu_cols = list(pipe.named_steps['preprocessor'].transformers_[2][1].named_steps['onehot'].get_feature_names_out(['GPU_simple']))\n",
    "all_feature_names = numeric_features + ohe_cols + gpu_cols\n",
    "importances = pd.Series(res.importances_mean, index=all_feature_names).sort_values(ascending=False)\n",
    "importances.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa252e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model for later use\n",
    "joblib.dump(pipe, 'laptop_price_model.joblib')\n",
    "print('Model saved to laptop_price_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4a459c",
   "metadata": {},
   "source": [
    "## Conclusion & Next steps\n",
    "\n",
    "- The notebook performs EDA, feature engineering, and trains a RandomForest model.\n",
    "- If you need higher R², try Gradient Boosting, LightGBM/XGBoost, hyperparameter tuning, target transformation (log), or more feature engineering.\n",
    "\n",
    "---\n",
    "\n",
    "**Files created:** `/mnt/data/Laptop_price_prediction.ipynb`, `laptop_price_model.joblib` (model saved after training when you run the notebook)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
